{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orHS5Xnfgb72",
        "outputId": "e8296e79-ad07-454b-c9dd-d857c7e10834"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install langdetect\n",
        "!pip install PyPDF2\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjTAGkMoot5D",
        "outputId": "5e2348b2-312a-4ee9-8904-47352dca0210"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.29.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksofltaLY0PZ"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"ExtraccionQuechuaCollao.ipynb\n",
        "\n",
        "Se instalan las librerias que se usaran\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import pdfplumber\n",
        "from langdetect import detect\n",
        "from google.colab import drive\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\"\"\"Se crea un acceso a Drive para acceder a los archivos\"\"\"\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Descargar recursos adicionales de NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\"\"\"Se define la funcion detect_language para detectar aquellas oraciones en español dentro de los documentos\"\"\"\n",
        "\n",
        "def detect_language(text):\n",
        "  try:\n",
        "    idioma = detect(text)\n",
        "    return idioma\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "\"\"\"Se define la función extract_text_from_pdf para extraer el texto de los documentos y filtrar solo aquellas oraciones en quechua\"\"\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    oraciones = text.split('.')\n",
        "    oraciones = [oracion.strip() for oracion in oraciones if oracion.strip()]\n",
        "    texto_quechua=[]\n",
        "    for oracion in oraciones:\n",
        "      idioma = detect_language(oracion)\n",
        "      if idioma != \"es\":\n",
        "          texto_quechua.append(oracion)\n",
        "    return \" \".join(texto_quechua)\n",
        "\n",
        "#def clean_text(text):\n",
        "    # Eliminar caracteres especiales y números\n",
        "    #text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    # Convertir a minúsculas\n",
        "    # text = text.lower()\n",
        "    # Tokenización\n",
        "    # tokens = word_tokenize(text)\n",
        "    # Eliminar stopwords en español\n",
        "    # stop_words = set(stopwords.words('spanish'))\n",
        "    # tokens = [word for word in tokens if word not in stop_words]\n",
        "    # return \" \".join(tokens)\n",
        "\n",
        "\"\"\"Se define la función create_corpus que recorre la carpeta en busca de los documentos para crear el corpus\"\"\"\n",
        "\n",
        "def create_corpus(pdf_folder):\n",
        "    corpus = []\n",
        "    for filename in os.listdir(pdf_folder):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(pdf_folder, filename)\n",
        "            text = extract_text_from_pdf(pdf_path)\n",
        "            #cleaned_text = clean_text(text)\n",
        "            corpus.append({'documento': text, 'variante': 'collao'})\n",
        "    return corpus\n",
        "\n",
        "\"\"\"Se define la ruta de la carpeta y se crea el corpus\"\"\"\n",
        "\n",
        "# Ruta al directorio que contiene los archivos PDF\n",
        "pdf_folder = \"/content/drive/My Drive/Corpus/collao\"\n",
        "\n",
        "# Crear el corpus\n",
        "corpus_collao = create_corpus(pdf_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extraer_lineas(inicio, fin, pdf_path, tipo):\n",
        "    paginas = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        num_paginas = len(reader.pages)\n",
        "\n",
        "        for pagina_num in range(inicio,fin+1):\n",
        "            pag = []\n",
        "            pagina = reader.pages[pagina_num]\n",
        "            contenido = pagina.extract_text()\n",
        "\n",
        "            # Dividir el contenido de la página en lineas\n",
        "            palabras = contenido.split('\\n')\n",
        "\n",
        "            #si la linea empieza con digito, entonces se elimina ese digito que es el indicador de pagina\n",
        "            if palabras[0].isdigit():\n",
        "              del palabras[0]\n",
        "            else:\n",
        "              # si por error la linea empieza con un numero que no deberia ir ahí, lo elimina\n",
        "              if palabras[0][0].isdigit():\n",
        "                palabras[0] = palabras[0][1:]\n",
        "\n",
        "            #une las palabras en una linea\n",
        "            pag.append(\" \".join(palabras))\n",
        "\n",
        "            #añade las lineas a el texto completo divido por paginas\n",
        "            paginas.append(\" \".join(pag[0].split()))\n",
        "\n",
        "    #une todo el texto y regresa un diccionario\n",
        "    return dict([('documento', \" \".join(paginas)), ('variante', tipo)])\n",
        "\n",
        "maychi = extraer_lineas(4,8, \"/content/drive/My Drive/Corpus/chanka/maychi.pdf\", \"Chanka\")\n",
        "chanka5 = extraer_lineas(4,8, \"/content/drive/My Drive/Corpus/chanka/antuku.pdf\", \"Chanka\")"
      ],
      "metadata": {
        "id": "uu39hgMzgkad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_lineas(inicio, fin, pdf_path, tipo):\n",
        "    paginas = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        num_paginas = len(reader.pages)\n",
        "\n",
        "        for pagina_num in range(inicio,fin+1):\n",
        "            pag = []\n",
        "            pagina = reader.pages[pagina_num]\n",
        "            contenido = pagina.extract_text()\n",
        "\n",
        "            # Dividir el contenido de la página en lineas\n",
        "            palabras = contenido.split('\\n')\n",
        "            #si la linea empieza con digito, entonces se elimina ese digito que es el indicador de pagina\n",
        "            if palabras[0].isdigit():\n",
        "              del palabras[0]\n",
        "            else:\n",
        "              #si la linea es el comienzo de un capitulo, el titulo se guardó al ultimo y se procede a eliminarlo\n",
        "              del palabras[-5:]\n",
        "\n",
        "            #une las palabras en una linea\n",
        "            pag.append(\" \".join(palabras))\n",
        "\n",
        "            #añade las lineas a el texto completo divido por paginas\n",
        "            paginas.append(\" \".join(pag[0].split()))\n",
        "\n",
        "    #une todo el texto y regresa un diccionario\n",
        "    return (\" \".join(paginas), tipo)\n",
        "    #return dict([('documento', \" \".join(paginas)), ('variante', tipo)])\n",
        "\n",
        "paginas = extraer_lineas(8,158, \"/content/drive/My Drive/Corpus/chanka/Chanka_Extraccion1.pdf\", \"Chanka\")"
      ],
      "metadata": {
        "id": "Y3HDBJOZgs63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_lineas(inicio, fin, pdf_path, tipo):\n",
        "    paginas = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        num_paginas = len(reader.pages)\n",
        "\n",
        "        for pagina_num in range(inicio,fin+1):\n",
        "            pag = []\n",
        "            pagina = reader.pages[pagina_num]\n",
        "            contenido = pagina.extract_text()\n",
        "\n",
        "            # Dividir el contenido de la página en lineas\n",
        "            palabras = contenido.split('\\n')\n",
        "            #si la linea empieza con digito, entonces se elimina ese digito que es el indicador de pagina\n",
        "            if palabras[0].isdigit():\n",
        "              del palabras[0]\n",
        "\n",
        "            #une las palabras en una linea\n",
        "            pag.append(\" \".join(palabras))\n",
        "\n",
        "            #añade las lineas a el texto completo divido por paginas\n",
        "            paginas.append(\" \".join(pag[0].split()))\n",
        "\n",
        "    #une todo el texto y regresa un diccionario\n",
        "    return dict([('documento', \" \".join(paginas)), ('variante', tipo)])\n",
        "\n",
        "paginas_2 = extraer_lineas(8,73, \"/content/drive/My Drive/Corpus/chanka/saberes.pdf\", \"Chanka\")"
      ],
      "metadata": {
        "id": "jRKYTTDJgvU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusChanka = [\n",
        "  maychi,\n",
        "  chanka5,\n",
        "  paginas,\n",
        "  paginas_2,\n",
        "]\n",
        "len(corpusChanka)"
      ],
      "metadata": {
        "id": "uGBKZZIlkZrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Fuente: Urin Qichwa VOCABULARIO PEDAGÓGICO QUECHUA SUREÑO\n",
        "reader = PdfReader(\"/content/drive/My Drive/Corpus/unificada/urin.pdf\")\n",
        "\n",
        "# Secciones con oraciones útiles\n",
        "pages_comm = reader.pages[17:47]  # Comunicación\n",
        "pages_pers = reader.pages[60:68]  # Personal social y ciencias\n",
        "pages_camp = reader.pages[109:131]  # Por campos semánticos\n",
        "\n",
        "document = \"\"\n",
        "\n",
        "### Sección Comunicación ###\n",
        "\n",
        "# Regex patterns\n",
        "pagination_pttrn = (\n",
        "    r\"Comunicación\\nvocabulario\\d+\\sParte\\sI\\s-\\sQuechua\\ssureño\"  # Datos de paginación\n",
        ")\n",
        "sentences_pttrn = r\"\\.[A-Za-z]?\\s*[A-Za-z]?\\s([^.]+\\.)\\s*‘([^‘’]+)’\\.\"  # Grupo1 oración quechua, grupo2 oración español encerrada en ‘’\n",
        "\n",
        "# Processing\n",
        "for page in pages_comm:\n",
        "    # Clean pagination\n",
        "    text = re.sub(pagination_pttrn, \"\", page.extract_text())\n",
        "\n",
        "    # Find quechua sentences\n",
        "    matches = re.findall(sentences_pttrn, text)\n",
        "    for m in matches:\n",
        "        sentence = m[0].replace(\"\\n\", \"\")\n",
        "        sentence = sentence.replace(\"/ \", \"/\")\n",
        "        document += sentence + \"\\n\"\n",
        "\n",
        "### Sección Personal social y ciencias ###\n",
        "\n",
        "# Regex patterns\n",
        "pagination_pttrn = r\"Personal\\ssocial\\sy\\sCiencia\\sy\\stecnología\\nvocabularioParte\\sII\\s-\\sQuechua sureño\\s\\d+\"  # Datos de paginación\n",
        "sentences_pttrn = r\"\\.[A-Za-z]?\\s*[A-Za-z]?\\s([^.]+\\.)\\s*‘([^‘’]+)’\\.\"  # Grupo1 oración quechua, grupo2 oración español encerrada en ‘’\n",
        "\n",
        "# Processing\n",
        "for page in pages_pers:\n",
        "    # Clean pagination\n",
        "    text = re.sub(pagination_pttrn, \"\", page.extract_text())\n",
        "\n",
        "    # Find quechua sentences\n",
        "    matches = re.findall(sentences_pttrn, text)\n",
        "    for m in matches:\n",
        "        sentence = m[0].replace(\"\\n\", \"\")\n",
        "        sentence = sentence.replace(\"/ \", \"/\")\n",
        "        document += sentence + \"\\n\"\n",
        "\n",
        "### Sección campos semánticos ###\n",
        "\n",
        "# Regex patterns\n",
        "pagination_pttrn = r\"Palabras\\spor\\scampos\\ssemánticos\\sdel\\squechua\\ssureño\\nUsadas\\sen\\sel\\sárea\\sde\\sPersonal\\ssocial\\sy\\sCiencia\\sy\\stecnologíaParte\\sIII\\s-\\sQuechua\\ssureño\\s\\d+\"  # Datos de paginación\n",
        "patron = r\"([a-z].*?\\.)\\s*\\n\\s*((?:[A-Z].*?\\.\\s*)+)\"  # Grupo1 oración en minúscula (palabras), grupo2 grupo de oraciones que empiezan en mayúsucula (ejemplos en quechua)\n",
        "\n",
        "# Processing\n",
        "for page in pages_camp:\n",
        "    # Clean pagination\n",
        "    text = re.sub(pagination_pttrn, \"\", page.extract_text())\n",
        "\n",
        "    # Find quechua sentences\n",
        "    matches = re.findall(patron, text, re.DOTALL)\n",
        "    for m in matches:\n",
        "        sentence = m[1].replace(\"\\n\", \"\")\n",
        "        sentence = sentence.replace(\"/ \", \"/\")\n",
        "        document += sentence + \"\\n\"\n",
        "\n",
        "\n",
        "corpusUnificado = {'documento': document, 'variante': 'unificada'}"
      ],
      "metadata": {
        "id": "JuNYC0KNihj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpusUnificado)"
      ],
      "metadata": {
        "id": "RTD7ZtaF2hMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=[]\n",
        "for doc in corpusChanka:\n",
        "  corpus.append(doc)\n",
        "corpus.append({'documento': corpusUnificado[\"documento\"], 'variante': corpusUnificado[\"variante\"]})\n",
        "for doc in corpus_collao:\n",
        "  corpus.append(doc)"
      ],
      "metadata": {
        "id": "lzDr6KVYk9JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "dVuMvWKc2yg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el corpus en un archivo de texto\n",
        "with open(\"corpus.txt\", \"w\") as f:\n",
        "    for document in corpus:\n",
        "        f.write(\"%s\\n\" % document)"
      ],
      "metadata": {
        "id": "r4cejbyk3UWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Número de documentos en el corpus:\", len(corpus))"
      ],
      "metadata": {
        "id": "cQTDvIXtEws8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el número de palabras de cada documento y del corpus\n",
        "num_palabras_documento = []\n",
        "for documento in corpus:\n",
        "    num_palabras = len(documento['documento'].split())\n",
        "    num_palabras_documento.append(num_palabras)\n",
        "    #print(f\"Número de palabras en el documento: {num_palabras}\")\n",
        "\n",
        "# Número total de palabras en el corpus\n",
        "total_palabras_corpus = sum(num_palabras_documento)\n",
        "print(\"Número total de palabras en el corpus:\", total_palabras_corpus)\n"
      ],
      "metadata": {
        "id": "bRSBVqN6FSpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Obtener el número de palabras de cada documento y del corpus\n",
        "num_palabras_documento = []\n",
        "for documento in corpus:\n",
        "    num_palabras = len(documento['documento'].split())\n",
        "    num_palabras_documento.append(num_palabras)\n",
        "\n",
        "# Número total de palabras en el corpus\n",
        "total_palabras_corpus = sum(num_palabras_documento)\n",
        "\n",
        "# Crear gráfico de barras para mostrar el número de palabras en cada documento\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, len(num_palabras_documento) + 1), num_palabras_documento, color='skyblue')\n",
        "plt.xlabel('Documento')\n",
        "plt.ylabel('Número de palabras')\n",
        "plt.title('Número de palabras en cada documento')\n",
        "plt.xticks(range(1, len(num_palabras_documento) + 1))\n",
        "\n",
        "# Agregar el número total de palabras en el gráfico\n",
        "plt.text(0.5, 0.95, f'Total de palabras: {total_palabras_corpus}', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes, fontsize=15, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n7dpW3YtIXeu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}