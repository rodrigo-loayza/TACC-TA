# -*- coding: utf-8 -*-
"""ExtraccionQuechuaCollao.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q0FCJzxmQVQY2d4VjZzisLy_kJm5VZZr

Se instalan las librerias que se usaran
"""

!pip install pdfplumber
!pip install langdetect

import os
import re
import nltk
import pdfplumber
from langdetect import detect
from google.colab import drive
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

"""Se crea un acceso a Drive para acceder a los archivos"""

drive.mount('/content/drive')

# Descargar recursos adicionales de NLTK
nltk.download('punkt')
nltk.download('stopwords')

"""Se define la funcion detect_language para detectar aquellas oraciones en español dentro de los documentos"""

def detect_language(text):
  try:
    idioma = detect(text)
    return idioma
  except:
    return None

"""Se define la función extract_text_from_pdf para extraer el texto de los documentos y filtrar solo aquellas oraciones en quechua"""

def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = ""
        for page in pdf.pages:
            text += page.extract_text()
    oraciones = text.split('.')
    oraciones = [oracion.strip() for oracion in oraciones if oracion.strip()]
    texto_quechua=[]
    for oracion in oraciones:
      idioma = detect_language(oracion)
      if idioma != "es":
          texto_quechua.append(oracion)
    return " ".join(texto_quechua)

#def clean_text(text):
    # Eliminar caracteres especiales y números
    #text = re.sub(r'[^a-zA-Z]', ' ', text)
    # Convertir a minúsculas
    # text = text.lower()
    # Tokenización
    # tokens = word_tokenize(text)
    # Eliminar stopwords en español
    # stop_words = set(stopwords.words('spanish'))
    # tokens = [word for word in tokens if word not in stop_words]
    # return " ".join(tokens)

"""Se define la función create_corpus que recorre la carpeta en busca de los documentos para crear el corpus"""

def create_corpus(pdf_folder):
    corpus = []
    for filename in os.listdir(pdf_folder):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(pdf_folder, filename)
            text = extract_text_from_pdf(pdf_path)
            #cleaned_text = clean_text(text)
            corpus.append({'documento': text, 'variante': 'collao'})
    return corpus

"""Se define la ruta de la carpeta y se crea el corpus"""

# Ruta al directorio que contiene los archivos PDF
pdf_folder = "/content/drive/My Drive/Corpus"

# Crear el corpus
corpus = create_corpus(pdf_folder)

corpus